{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abbiu/School/blob/main/BAIT_508_Indsutry_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data**"
      ],
      "metadata": {
        "id": "QxzmshKbjN_k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "q7vvftP06vEg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e9ac48-13af-4f7f-c8a1-8a63c660ab50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import string\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the data from Google Drive\n",
        " # **** (cite AI usage!) ChatGPT 5 - on changing Google Drive url for loading\n",
        "# data\n",
        "url_mg = 'https://drive.google.com/uc?export=download&id=1jinQrkO-FevyrUlFDEV2QJiKtTtEjzrt'\n",
        "url_pf = 'https://drive.google.com/uc?export=download&id=1-gbc5tMK6t1BB9_yFYMo4YIySHMNtGkq'\n"
      ],
      "metadata": {
        "id": "Ye9alG70YCOd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mg = pd.read_csv(url_mg)\n",
        "df_pf = pd.read_csv(url_pf)\n"
      ],
      "metadata": {
        "id": "xizcd0i8aybq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_mg.head()"
      ],
      "metadata": {
        "id": "Iur7f2Y9bY6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf.head()"
      ],
      "metadata": {
        "id": "rHyIdDp9hSJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1 Quantitative Analysis of the Food Store Industry**"
      ],
      "metadata": {
        "id": "ZOylinPkjctP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Data Filtering**"
      ],
      "metadata": {
        "id": "sPU-GSjKlN1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf_final = df_pf[df_pf[\"sic\"].astype(str).str.startswith('54', na=False)]\n",
        "display(df_pf_final.head())"
      ],
      "metadata": {
        "id": "T5aENfwxhq5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_pf_final.isna().sum())"
      ],
      "metadata": {
        "id": "3teBr2jor_O7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a. Number of unique firm-year observations\n",
        "num_firm_year = df_pf_final.groupby(['gvkey', 'fyear']).size().shape[0]\n",
        "print(\"Number of unique firm-year observations:\", num_firm_year)\n",
        "\n",
        "# b. Number of unique firms\n",
        "num_firms = df_pf_final.groupby('gvkey').ngroups\n",
        "print(\"Number of unique firms:\", num_firms)\n",
        "\n",
        "# c. Number of firms with records over all 27 years (1994-2020)\n",
        "# Count how many years each firm appears\n",
        "years_per_firm = (\n",
        "    df_pf_final[['gvkey', 'fyear']]\n",
        "    .drop_duplicates()\n",
        "    .groupby('gvkey')['fyear']\n",
        "    .count()\n",
        ")\n",
        "print(years_per_firm)\n",
        "# Firms that have all 27 years\n",
        "firms_all_years = years_per_firm[years_per_firm == 27].count()\n",
        "print(\"Number of firms with records for all 27 years:\", firms_all_years)"
      ],
      "metadata": {
        "id": "XhBlFNUm8a4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 Preliminary Analysis**"
      ],
      "metadata": {
        "id": "zrmAUZK_h7op"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1\n",
        " What are the top 10 firms with the highest stock price?"
      ],
      "metadata": {
        "id": "-4dt_F7bkWju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf_final[df_pf_final[\"fyear\"] == 2020].sort_values(by=\"prcc_c\", ascending=False).head(10)\n"
      ],
      "metadata": {
        "id": "hEcYYpszjkQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2\n",
        " What are the top 10 firms with the highest sales (column \"sale\") in the entire history of the dataset?"
      ],
      "metadata": {
        "id": "ZSRIdSGolz1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf.sort_values(by=\"sale\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "acy8fiE7l4tk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 3\n",
        " What is the geographical distribution (column \"location\") of all the firms? In other words, how many firms are there in each location? Please list the top 10 locations"
      ],
      "metadata": {
        "id": "JanzEtotqlIi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf_final.groupby(\"location\").count().sort_values(by=\"location\", ascending=False).head(10)"
      ],
      "metadata": {
        "id": "EO7Ez3FCqoaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Question 4\n",
        "\n",
        "Create a line chart to show the average stock price (column \"prcc_c\") in the selected sector(s) across the years. If you have selected multiple sectors, draw multiple lines to show them separately."
      ],
      "metadata": {
        "id": "Zzm2A2sYibbl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mean_df_pf = df_pf_final.groupby(by='fyear').mean(numeric_only=True).reset_index()\n",
        "mean_df_pf.head()"
      ],
      "metadata": {
        "id": "xuPYggvLih0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Please execute the cell above first to define `mean_df_pf`\n",
        "plt.plot(mean_df_pf['fyear'],mean_df_pf['prcc_c'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average Stock Price')\n",
        "plt.title('Average Stock Price in the Food Store Industry over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EWRdTR22iz-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quesiton 5\n",
        "Which firm was affected the most by the 2008 Financial Crisis, as measured by the percentage drop in stock price from 2007 to 2008?"
      ],
      "metadata": {
        "id": "33EtJsHdi5fg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf_07_08 =  df_pf_final[(df_pf_final['fyear']==2007) | (df_pf_final['fyear']==2008)]"
      ],
      "metadata": {
        "id": "CpTe0xdNjEj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " #reshape the dataframe\n",
        "stock_price_df = df_pf_07_08.pivot(index='conm',columns = 'fyear', values = 'prcc_c').reset_index()\n",
        "stock_price_df['change_percentage'] = (stock_price_df[2008]-stock_price_df[2007])/stock_price_df[2007]\n",
        "stock_price_df.head()"
      ],
      "metadata": {
        "id": "XK28wGtnjHGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#find the company that was affected the most\n",
        "stock_price_df.sort_values('change_percentage').iloc[0,0]"
      ],
      "metadata": {
        "id": "2_KyqOzWjRhj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quesiton 6\n",
        "Plot the average Return on Assets (ROA) for the firms located in the “USA” across the years. ROA is calculated as ni/asset."
      ],
      "metadata": {
        "id": "ZBzRVlPYjWMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_pf_final['location'].unique()"
      ],
      "metadata": {
        "id": "rPH8iNC1jZPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "USA_df = df_pf_final[df_pf_final['location']=='USA']\n",
        "USA_df.head()\n",
        "# roa already exists in the given data frame\n",
        "#USA_df['ROA'] = USA_df['ni']/USA_df['asset']"
      ],
      "metadata": {
        "id": "N52VJj-mjdoL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roa_USA_df = USA_df.groupby('fyear').mean(numeric_only = True).reset_index()[['fyear','roa']]"
      ],
      "metadata": {
        "id": "4lhRmJLgjl_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(roa_USA_df['fyear'],roa_USA_df['roa'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Average ROA')\n",
        "plt.title('Average ROA of the US Food Store Industry over Time')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kSLhaw8Djql2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2 Text Analysis on the Food Store Industry**"
      ],
      "metadata": {
        "id": "gs0u86HnkUSM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Text Cleaning**"
      ],
      "metadata": {
        "id": "O6E9lbC7l1DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## !!! Cite AI usage for using drop box link to load large size data - ChatGPT 5\n",
        "url_10k = 'https://www.dropbox.com/scl/fi/8p9klgbjasek74xfqaydi/2020_10K_item1_full.csv?rlkey=mvsehcwgkevlt5t3fqjnirt2e&st=oyy6365c&dl=1'"
      ],
      "metadata": {
        "id": "WNCOhbJZikUo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_10k = pd.read_csv(url_10k)"
      ],
      "metadata": {
        "id": "GANw4I-oixlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_10k.head()"
      ],
      "metadata": {
        "id": "7pC4gh0SYDhU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the texts and store the new values in column *item_1*"
      ],
      "metadata": {
        "id": "EhqHkfZaqyAL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!!! Cite, code directly copied from NLP Part 2 Keyword Analysis Lecture notebook !!!\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "sw = stopwords.words('english')\n",
        "\n",
        "def clean_text(text):\n",
        "    ''' This function takes a string as input and\n",
        "        returns a cleaned version of the string\n",
        "        Specifically, it makes the string into lower case and remove punctuations\n",
        "    '''\n",
        "    text_lower = text.lower() # make it lowercase\n",
        "    text_no_punctuation = text_lower.translate(translator) # remove punctuation\n",
        "    clean_words = [w for w in text_no_punctuation.split() if w not in sw] # remove stopwords\n",
        "    return ' '.join(clean_words)"
      ],
      "metadata": {
        "id": "R3XdWySpqRTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# be careful with re-running this, took me around 3 minutes to execute\n",
        "df_10k['item_1'] = df_10k['item_1_text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "GsHHZLUFuM8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_10k.head()"
      ],
      "metadata": {
        "id": "j4kLkK5Xqsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Keyword Analysis**"
      ],
      "metadata": {
        "id": "8w2JiHCqmGJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create a new DataFrame that includes only firms in your selected industry sector(s). Ensure that you merge the 10-K data with the previous \"public_firm.csv\" data using an inner join.\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "5H_wG4Z-FHvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# inner join 2 datasets... leaving the rest to you guys!\n",
        "df_sector = pd.merge (df_10k, df_pf_final, left_on=['gvkey', 'year'], right_on=['gvkey','fyear'], how='inner')\n",
        "\n",
        "\n",
        "# Step 1: Select sector(s)\n",
        "#df_sector = df_merged[df_merged['sic'].astype(str).str.startswith('54')]\n",
        "#df_sector = df_sector[df_sector['item_1'].str.strip() != \"\"].reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "9yZCBoJmfb_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df_sector.drop_duplicates(subset=['gvkey'], inplace=True)\n",
        "#df_sector['name']\n"
      ],
      "metadata": {
        "id": "FolnSxmsgiFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Generate the top 10 keywords for each firm based on two different methods: word counts and TF-IDF score."
      ],
      "metadata": {
        "id": "k5lr8BDMFtcY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "def get_keywords_counts(document_list, top_n=10):\n",
        "    \"\"\"\n",
        "    Get top keywords for each document using raw word counts.\n",
        "    Returns:\n",
        "    - list of dicts: {\"words\": \"word1 word2 ... word10\", \"counts\": [(word, count), ...]}\n",
        "    \"\"\"\n",
        "    results = []\n",
        "    for text in document_list:\n",
        "        words = text.split()  # simple tokenization\n",
        "        c = Counter(words)\n",
        "        top_pairs = c.most_common(top_n)\n",
        "        top_words = ' '.join([w for w, _ in top_pairs])\n",
        "\n",
        "        results.append({\"words\": top_words, \"counts\": top_pairs})\n",
        "    return results\n",
        "\n",
        "# Apply to your sector\n",
        "documents = df_sector['item_1'].fillna(\"\").tolist()\n",
        "df_sector['count_results'] = get_keywords_counts(documents, top_n=10)\n",
        "\n",
        "# Just words for easier viewing\n",
        "df_sector['top_keywords_counts'] = df_sector['count_results'].apply(lambda x: x['words'])\n",
        "\n",
        "# Show sample firms with their top keywords (by raw counts)\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df_sector[['name', 'gvkey', 'sic', 'top_keywords_counts']].head(13))\n"
      ],
      "metadata": {
        "id": "njDYhBL7hAZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "def get_keywords_tfidf(document_list, top_n=10):\n",
        "    \"\"\"\n",
        "    Get top keywords for each document using TF-IDF.\n",
        "    Returns:\n",
        "    - list of dicts: {\"words\": \"word1 word2 ... word10\", \"scores\": [(word, score), ...]}\n",
        "    \"\"\"\n",
        "    vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "    tfidf_matrix = vectorizer.fit_transform(document_list)\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    results = []\n",
        "    for i in range(len(document_list)):\n",
        "        row = tfidf_matrix[i, :].tocoo()\n",
        "        tfidf_scores = list(zip(row.col, row.data))\n",
        "        sorted_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "        # Top N words and scores\n",
        "        top_pairs = [(feature_names[idx], float(score)) for idx, score in sorted_scores[:top_n]]\n",
        "        top_words = ' '.join([w for w, _ in top_pairs])\n",
        "\n",
        "        results.append({\"words\": top_words, \"scores\": top_pairs})\n",
        "\n",
        "    return results\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "get_keywords_tfidf(corpus)\n",
        "\n",
        "\n",
        "# Apply\n",
        "documents = df_sector['item_1'].fillna(\"\").tolist()\n",
        "df_sector['tfidf_results'] = get_keywords_tfidf(documents, top_n=10)\n",
        "\n",
        "# Show just words in the main view\n",
        "df_sector['top_keywords_tfidf'] = df_sector['tfidf_results'].apply(lambda x: x['words'])\n",
        "\n",
        "# See first few firms with keywords\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "display(df_sector[['name', 'gvkey', 'sic', 'top_keywords_tfidf']].head(13))\n"
      ],
      "metadata": {
        "id": "EdEoQIo6-8kW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gene\n",
        "def get_keywords_tfidf(document_list):\n",
        "    '''\n",
        "    This function gets a list of documents as input and returns a list of top 10 keywords for each document using TF-IDF scores.\n",
        "    Input: A list of documents (text)\n",
        "    Output: The corresponding top 10 keywords for each document based on tf-idf values\n",
        "    '''\n",
        "    vectorizer = TfidfVectorizer() # Step 1: Create a TF-IDF vectorizer\n",
        "    tfidf_matrix = vectorizer.fit_transform(document_list) # Step 2: Calculate the TF-IDF matrix\n",
        "    feature_names = vectorizer.get_feature_names_out() # Step 3: Get feature names (words)\n",
        "\n",
        "    # Step 4: Extract top 10 keywords for each document\n",
        "    top_keywords = [] # accumulator\n",
        "    for i in range(len(document_list)):\n",
        "        feature_index = tfidf_matrix[i, :].nonzero()[1]\n",
        "        feature_value = [tfidf_matrix[i, x] for x in feature_index]\n",
        "        tfidf_scores = zip(feature_index, feature_value)\n",
        "        sorted_tfidf_scores = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)\n",
        "        top_keywords.append(' '.join([feature_names[i] for i, _ in sorted_tfidf_scores[:10]]))\n",
        "\n",
        "        if i % 200 == 199:\n",
        "            print(f'Processed {i+1}/{len(document_list)} documents.')\n",
        "\n",
        "    return top_keywords\n",
        "\n",
        "corpus = [\n",
        "    'This is the first document.',\n",
        "    'This document is the second document.',\n",
        "    'And this is the third one.',\n",
        "    'Is this the first document?',\n",
        "]\n",
        "get_keywords_tfidf(corpus)\n",
        "\n",
        "docs = df_sector['item_1'].tolist()\n",
        "len(docs)\n",
        "\n",
        "# This process will take several minutes.\n",
        "tfidf_keywords = get_keywords_tfidf(docs)\n",
        "\n",
        "# add a new column in the dataframe\n",
        "df_sector['top_keyword_tfidf'] = tfidf_keywords\n",
        "\n",
        "# check what tfidf_keywords contain.\n",
        "print(type(tfidf_keywords))\n",
        "print(len(tfidf_keywords))\n"
      ],
      "metadata": {
        "id": "5xApTbsGl10h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_keywords"
      ],
      "metadata": {
        "id": "WNtxpGbRGe9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pick which firm to analyze\n",
        "n = 8  # change this index to any firm number\n",
        "\n",
        "# Get firm info\n",
        "firm_name = df_sector['name'].iloc[n]\n",
        "gvkey = df_sector['gvkey'].iloc[n]\n",
        "\n",
        "# Show the words\n",
        "print(f\"Top 10 words for {firm_name} (gvkey={gvkey}):\")\n",
        "print(df_sector['top_keywords_tfidf'].iloc[n])\n",
        "\n",
        "# Dig into the dict to see scores\n",
        "print(\"\\nWith scores:\")\n",
        "for word, score in df_sector['tfidf_results'].iloc[n]['scores']:\n",
        "    print(f\"{word}: {score:.4f}\")"
      ],
      "metadata": {
        "id": "7pVUiHusD273"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Create two wordclouds to visualize the keywords across all firms in the selected sector(s): one based on the word counts and another based on the TF-IDF scores."
      ],
      "metadata": {
        "id": "0rJz-IXWGBDj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Build one big string from the text column\n",
        "text1 = ' '.join(keyworddf['top_keywords_counts'].astype(str).tolist())\n",
        "\n",
        "wc = WordCloud(width=800, height=400, max_font_size=100, background_color='white')\n",
        "wordcloud1 = wc.generate(text1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.imshow(wordcloud1, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title(\"WordCloud : Sector-Level Word Counts\", fontsize=16, pad=10)\n",
        "plt.tight_layout()\n",
        "plt.savefig('keyword_all.png', dpi=200, bbox_inches='tight')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "A2gxfMNJ_TmG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import relavant packages\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "# prepare text# prepare text\n",
        "text1 = ' '.join(df_sector['item_1'].tolist())\n",
        "# lower max_font_size\n",
        "wordcloud1 = WordCloud(width=800, height=400, max_font_size=100, background_color='white').generate(text1) # note that text is a string, not a list\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.title(\"WordCloud : Sector-Level Word Counts\", fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.imshow(wordcloud1)\n",
        "plt.savefig('keyword_all.png') # save as PNG file\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KC3_WGeO8Pon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- prepare documents (one doc per firm/row) ---\n",
        "texts = df_sector['item_1'].dropna().astype(str).tolist()\n",
        "\n",
        "# --- TF-IDF: build term weights across the sector ---\n",
        "# You can tweak: stop_words, token_pattern, ngram_range, max_features\n",
        "vectorizer = TfidfVectorizer(\n",
        "    lowercase=True,\n",
        "    stop_words='english',              # remove common English words\n",
        "    token_pattern=r'(?u)\\b[a-zA-Z]{3,}\\b',  # words with >=3 letters\n",
        "    ngram_range=(1, 1),                # unigrams; set to (1,2) to include bigrams\n",
        "    max_features=8000                  # cap vocab size (optional)\n",
        ")\n",
        "X = vectorizer.fit_transform(texts)        # shape: (n_docs, n_terms)\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "\n",
        "# Sum TF-IDF across all documents to get sector-level importance per term\n",
        "sector_tfidf = np.asarray(X.sum(axis=0)).ravel()   # shape: (n_terms,)\n",
        "freqs = dict(zip(terms, sector_tfidf))\n",
        "\n",
        "# (Optional) keep only the top-N weighted terms for a cleaner cloud\n",
        "N = 1000\n",
        "freqs = dict(sorted(freqs.items(), key=lambda kv: kv[1], reverse=True)[:N])\n",
        "\n",
        "# --- WordCloud from TF-IDF frequencies ---\n",
        "wordcloud1 = WordCloud(\n",
        "    width=800,\n",
        "    height=400,\n",
        "    max_font_size=100,\n",
        "    background_color='white',\n",
        "    prefer_horizontal=0.9,\n",
        "    normalize_plurals=False\n",
        ").generate_from_frequencies(freqs)\n",
        "\n",
        "# --- plot & save ---\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.axis('off')\n",
        "plt.imshow(wordcloud1, interpolation='bilinear')\n",
        "plt.savefig('keyword_all_tfidf.png', bbox_inches='tight', dpi=150)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "JA6bwBvahFZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Word Embedding**\n",
        "\n"
      ],
      "metadata": {
        "id": "o3jKaaevmMoe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.1 Importing and descriptive analytics**"
      ],
      "metadata": {
        "id": "9CGOnTyORDeq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Install and import gensim"
      ],
      "metadata": {
        "id": "lQH5GpaVF25l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip3 install gensim\n",
        "!pip uninstall -y gensim numpy\n",
        "!pip install gensim==4.3.3 numpy==1.26.4"
      ],
      "metadata": {
        "id": "YuHfCo6auouF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec"
      ],
      "metadata": {
        "id": "3DwKvKs6uogp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of list of words\n",
        "docs = [row.split() for row in df_10k['item_1']]"
      ],
      "metadata": {
        "id": "lIwFlZWQQh5z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(docs))"
      ],
      "metadata": {
        "id": "hJ_CG72aQohk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training and load word2vec model"
      ],
      "metadata": {
        "id": "hOplkI2PGEO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training word2vec model using the list of words in `sent`\n",
        "model = Word2Vec(docs, min_count=5, vector_size=50, workers=3, window=5, sg = 1)"
      ],
      "metadata": {
        "id": "S0aYEjfDRMdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save the model for future use; you don't need to train Word2Vec for multiple times\n",
        "model.save(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "c7q3jmpmTIps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model from stored file\n",
        "model = Word2Vec.load(\"word2vec.model\")"
      ],
      "metadata": {
        "id": "IuhcQy_0TKzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3.2 Word similarity**\n"
      ],
      "metadata": {
        "id": "scV_GTtYUFy0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('company')"
      ],
      "metadata": {
        "id": "qbKSwqjOUoOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- shares, chief, officer, directors seems to be the most relevant to company, which makes sense.\n",
        "- 7, 2016, 1000 suggests that they are oftenly mentioned in the report like \"the company year 2016\" or \"1000 shares owned\", etc.\n",
        "- the model think that these words are relevant to \"company\" at > 0.95 score"
      ],
      "metadata": {
        "id": "8d5GYLMdGNlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('store')"
      ],
      "metadata": {
        "id": "6JyEmHukUygQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('customers')"
      ],
      "metadata": {
        "id": "LsCkKqydUzKH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most similar words"
      ],
      "metadata": {
        "id": "i5rYgKsUUvlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('company')"
      ],
      "metadata": {
        "id": "KpmDZx_eU2bm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('store')"
      ],
      "metadata": {
        "id": "mryPGpJYFN1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.wv.most_similar('customers')"
      ],
      "metadata": {
        "id": "cOt3A8oJFOPA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Comprehensive Analysis of One Sample Firm**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "4HuUp-VxmZzQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let say we picked CASEYS GENERAL STORES INC"
      ],
      "metadata": {
        "id": "mWrxk_jkoo77"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 1 -  Find the focal firm’s competing firms"
      ],
      "metadata": {
        "id": "yOvrlp4Kr2-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up DocumentSimilarity"
      ],
      "metadata": {
        "id": "c7hFOrfxpL04"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentSimilarity:\n",
        "\n",
        "    def __init__(self, model, gvkeys, conm, keywordslist):\n",
        "        '''\n",
        "        Initialize the class\n",
        "        model: the word2vec model\n",
        "        gvkeys: a list/pandas series of unique firm identifiers\n",
        "        conm: a list/pandas series of company names\n",
        "        keywordslist: a list of keywords\n",
        "\n",
        "        gvkeys and keywordslist should be of the same length\n",
        "        '''\n",
        "\n",
        "        assert len(gvkeys) == len(keywordslist) == len(conm), \"gvkeys, conm, keywordslist should should be of the same length\"\n",
        "\n",
        "        # store the information\n",
        "        self.model = model\n",
        "        self.firms = list(gvkeys)\n",
        "        self.conm = list(conm)\n",
        "        self.keywordslist = [x.split() for x in list(keywordslist)]\n",
        "\n",
        "        # generate document embedding\n",
        "        self.document_embeddings = [self.model.wv.get_mean_vector(x) for x in self.keywordslist]\n",
        "\n",
        "        # convert to array to facilitate computation, normalize it\n",
        "        self.document_array = np.array(self.document_embeddings)\n",
        "        self.document_array = self.document_array / np.linalg.norm(self.document_array, axis=1)[:, np.newaxis]\n",
        "\n",
        "    def get_firm_embedding(self, firm):\n",
        "        '''Given the firm unique identifier, return the embedding of this firm'''\n",
        "\n",
        "        return self.document_embeddings[self.firms.index(firm)]\n",
        "\n",
        "    def similarity(self, firm1, firm2):\n",
        "        '''Given two firms' unique identifiers, return the similarity between the two firms'''\n",
        "        firm1 = self.document_embeddings[self.firms.index(firm1)]\n",
        "        firm2 = self.document_embeddings[self.firms.index(firm2)]\n",
        "\n",
        "        return np.dot(firm1, firm2) / (np.linalg.norm(firm1) * np.linalg.norm(firm2))\n",
        "\n",
        "    def most_similar(self, firm, topn = 5):\n",
        "        '''Given one firm unique identifier, return the topn similar firms to it\n",
        "        firm: firm unique identifier\n",
        "        topn: the number of firms to return\n",
        "        '''\n",
        "\n",
        "        v = self.document_embeddings[self.firms.index(firm)]\n",
        "        v = v / np.linalg.norm(v)\n",
        "\n",
        "        cosine_similarities = np.dot(self.document_array, v)\n",
        "\n",
        "        # find the index of the top n companies\n",
        "        sorted_indices = np.argsort(-cosine_similarities)\n",
        "        largest_n_indices = sorted_indices[:topn + 1]\n",
        "\n",
        "        return [(self.firms[x], self.conm[x], cosine_similarities[x]) for x in largest_n_indices[1:]]"
      ],
      "metadata": {
        "id": "svdLlM2RcPny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe changing this to the final merge df to get the most similar from the whole dataset(*but collab gonna crash :< )\n",
        "docsim = DocumentSimilarity(model = model, gvkeys=df_sector['gvkey'], conm = df_sector['name'],\n",
        "                       keywordslist = df_sector['top_keywords_tfidf'])\n",
        "type(docsim)"
      ],
      "metadata": {
        "id": "pe7GDL4uLOab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sector[['gvkey','name']].drop_duplicates().head(10)"
      ],
      "metadata": {
        "id": "-2EVxoQioOMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Find the most similar firms to CASEYS GENERAL STORES INC"
      ],
      "metadata": {
        "id": "NXQroDoopRc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CASEYS GENERAL STORES INC (gvkey = 2807)\n",
        "docsim.get_firm_embedding(firm = 2807)"
      ],
      "metadata": {
        "id": "Rii4vn6Ju2Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docsim.most_similar(firm = 2807, topn = 10)"
      ],
      "metadata": {
        "id": "wF8bEhESoYGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 2 - Descriptive analytics of the chosen firm"
      ],
      "metadata": {
        "id": "baSd6SwOsOV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_chosen = df_pf_final[df_pf_final['gvkey']==2807]"
      ],
      "metadata": {
        "id": "1fkUzh5zsb1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chosen.info()"
      ],
      "metadata": {
        "id": "9lLdnKcIUIYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chosen.corr(numeric_only=True)"
      ],
      "metadata": {
        "id": "6whMPlNQXgty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_chosen['fyear'],df_chosen['prcc_c'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Stock price')\n",
        "plt.title('Stock price movement by year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NTJpAaHdUK47"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- we see that net income(ni), asset and sale has strong correlation to the stock price."
      ],
      "metadata": {
        "id": "lYWuD1ovYzu1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_chosen['fyear'],df_chosen['ni'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Net Income')\n",
        "plt.title('Net Income by year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VQZJXW2UaMwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_chosen['fyear'],df_chosen['roa'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Return on Asset')\n",
        "plt.title('ROA by year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qRSVUm8Lhs5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Option 3 - Measure firm liquility\n",
        "\n",
        "Assume that ch(Cash & Short- Term Investment) and Asset are reported in millions:\n",
        "\n",
        "$$\n",
        "\\text{Liquidity} = \\frac{\\text{Cash & Short-Term Investments (ch)}}{\\text{Total Assets}}\n",
        "$$"
      ],
      "metadata": {
        "id": "LeiqR2tQlYnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_chosen['liquidity'] = df_chosen['ch'].values/df_chosen['asset'].values\n"
      ],
      "metadata": {
        "id": "Xe8Baen3ldfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_chosen['liquidity'].describe()"
      ],
      "metadata": {
        "id": "1Hodu-tAn0WW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- on average, on 4% of total assets were in cash & short-term investment\n",
        "- the highest cash ratio on assets is 12.67%\n",
        "- the lowest is 0.7%\n"
      ],
      "metadata": {
        "id": "ZuLee1NgoDpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df_chosen['fyear'],df_chosen['liquidity'])\n",
        "plt.xlabel('Year')\n",
        "plt.ylabel('Liquidity ratio')\n",
        "plt.title('Liquidity power over the year')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TWFhCiEKn_Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Testing to be revised\n",
        "1. **1994–2000:** Liquidity ratio remains very low (<0.03), with small fluctuations. The firm likely reinvested heavily in operations rather than holding large cash reserves.  \n",
        "2. **2001–2007:** Clear upward trend, peaking around 0.12–0.13 before 2008. Suggests precautionary cash accumulation leading up to the financial crisis.  \n",
        "3. **2008–2010:** Sharp decline in liquidity. Indicates the firm may have drawn down cash buffers or that assets grew faster than cash during the global financial crisis.  \n",
        "4. **2011–2016:** Volatile liquidity (0.02–0.05). Reflects unstable cash management, possibly due to uneven profitability, investments, or financing needs.  \n",
        "5. **2017–2020:** Recovery toward 0.07–0.08, showing the firm rebuilt its liquidity position and stabilized operations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ntMCWPM8os49"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_chosen[['liquidity','prcc_c','roa']].corr(method='pearson')"
      ],
      "metadata": {
        "id": "FeAullTSqF1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing, remove if necessary\n",
        "# Normalize each variable to 0–1 scale for comparability\n",
        "df_plot = df_chosen.copy()\n",
        "df_plot['liq_norm'] = df_plot['liquidity'] / df_plot['liquidity'].max()\n",
        "df_plot['roa_norm'] = df_plot['roa'] / df_plot['roa'].max()\n",
        "df_plot['price_norm'] = df_plot['prcc_c'] / df_plot['prcc_c'].max()\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(df_plot['fyear'], df_plot['liq_norm'], marker='o', label='Liquidity Ratio')\n",
        "plt.plot(df_plot['fyear'], df_plot['roa_norm'], marker='s', label='ROA')\n",
        "plt.plot(df_plot['fyear'], df_plot['price_norm'], marker='^', label='Stock Price')\n",
        "\n",
        "plt.title(\"Liquidity vs ROA and Stock Price (Normalized) Over Time\")\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Normalized Value (0–1)\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uf1sujo7q4et"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}